such that words used in similar contexts (e.g., “frog”, “toad”) have similar vectors. Word2vec uses
neural networks trained either to predict the current word from surrounding words (continuous
bag of words), or to predict the surrounding words given the current word (continuous skip-gram).
GloVe rather applies a regression model over a matrix of co-occurrence probabilities of word pairs.
Embeddings generated by both approaches are widely used in natural language processing tasks.

Another approach for graph embeddings is thus to leverage proven approaches for language
embeddings. However, while a graph consists of an unordered set of sequences of three terms
(ie., a set of edges), text in natural language consists of arbitrary-length sequences of terms (i.e.,
sentences of words). Along these lines, RDF2Vec [416] performs (biased [91]) random walks on
the graph and records the paths (the sequence of nodes and edge labels traversed) as “sentences”,
which are then fed as input into the word2vec [330] model. An example of such a path extracted
from Figure 21 might be, for example, (San Pedro)—bus->(Calama)- flight > (Iquique)- flight (Santiago), where
the paper experiments with 500 paths of length 8 per entity. RDF2Vec also proposes a second mode
where sequences are generated for nodes from canonically-labelled sub-trees of which they are a
root node, where the paper experiments with sub-trees of depth 1 and 2. Conversely, KGloVe [92]
is based on the GloVe model. Much like how the original GloVe model [383] considers words that
co-occur frequently in windows of text to be more related, KGloVe uses personalised PageRank”? to
determine the most related nodes to a given node, whose results are then fed into the GloVe model.

 

 

5.2.5 Entailment-aware models. The embeddings thus far consider the data graph alone. But what
if an ontology or set of rules is provided? Such deductive knowledge could be used to improve the
embeddings. One approach is to use constraint rules to refine the predictions made by embeddings;
for example, Wang et al. [520] use functional and inverse-functional definitions as constraints
(under UNA) such that, for example, if we define that an event can have at most one value for
venue, this is used to lower the plausibility of edges that would assign multiple venues to an event.
More recent approaches rather propose joint embeddings that consider both the data graph and
rules when computing embeddings. KALE [200] computes entity and relation embeddings using a
translational model (specifically TransE) that is adapted to further consider rules using t-norm fuzzy
logics. With reference to Figure 21, consider a simple rule '2\~bus-+14! => Wxt-connects to?! We can
use embeddings to assign plausibility scores to new edges, such as e1: (Piedras Rojas) bus—>(Moon Valley),
We can further apply the previous rule to generate a new edge eg: (Piedras Rojas)-connects to-»(Moon Valley)
from the predicted edge e,. But what plausibility should we assign to this second edge? Letting p;
and pz be the current plausibility scores of e; and e; (initialised using the standard embedding), then
t-norm fuzzy logics suggests that the plausibility be updated as p;p2 — pi + 1. Embeddings are then
trained to jointly assign larger plausibility scores to positive examples versus negative examples
of both edges and ground rules. An example of a positive ground rule based on Figure 21 would
| bus» Gan Pedro) => (Arica)-connects to->(an Pedro), Negative ground rules randomly replace the
relation in the head of the rule; for example, bus—»(San Pedro) =» (Arica)—flight—>Gan Pedro). Guo
et al. [201] later propose RUGE, which uses a joint model over ground rules (possibly soft rules
with confidence scores) and plausibility scores to align both forms of scoring for unseen edges.
Generating ground rules can be costly. An alternative approach, called FSL [120], observes that
in the case of a simple rule, such as '~bus-y! = Uxt-connects tot! the relation embedding
bus should always return a lower plausibility than connects to. Thus, for all such rules, FSL
proposes to train relation embeddings while avoiding violations of such inequalities. While relatively
straightforward, FSL only supports simple rules, while KALE also supports more complex rules.

 

 

5Intuitively speaking, personalised PageRank starts at a given node and then determines the probability of a random walk
being at a particular node after a given number of steps. A higher number of steps converges towards standard PageRank
emphasising global node centrality, while a lower number emphasises proximity/relatedness to the starting node.

41