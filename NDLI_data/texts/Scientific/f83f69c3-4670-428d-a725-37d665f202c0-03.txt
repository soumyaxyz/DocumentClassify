ments and answers from machine comprehension
datasets. Seq2Seq was also applied on neural sen-
tence simplification (Zhang and Lapata, 2017) and
paraphrase generation tasks (Xu et al., 2018).

3 Model Architecture

Given a piece of source text, our objective is to
generate a variable number of multi-word phrases.
To this end, we opt for the sequence-to-sequence
(Seq2Seq) (Sutskever et al., 2014) framework as
the basis of our model, combined with attention
and pointer softmax mechanisms in the decoder.

Since each data example contains one source
text sequence and multiple target phrase sequences
(dubbed OnE2Many, and each sequence can be
of multi-word), two paradigms can be adopted for
training Seq2Seq models. The first one (Meng
et al., 2017) is to divide each ONE2MaNy data ex-
ample into multiple ONzE2ONE examples, and the
resulting models (e.g., CopyRNN) can generate
one phrase at once and must rely on beam search
technique to produce more unique phrases.

To enable models to generate multiple phrases
and control the number to output, we propose the
second training paradigm ONE2S£Q, in which we
concatenate multiple phrases into a single sequence
with a delimiter (sep), and this concatenated se-
quence is then used as the target for sequence gen-
eration during training. An overview of the model’s
structure is shown in Figure 1.!

Notations

In the following subsections, we use w to denote
input text tokens, x to denote token embeddings,
h to denote hidden states, and y to denote output
text tokens. Superscripts denote time-steps in a
sequence, and subscripts e and d indicate whether a
variable resides in the encoder or the decoder of the
model, respectively. The absence of a superscript
indicates multiplicity in the time dimension. L
refers to a linear transformation and L/ refers to
it followed by a non-linear activation function f.
Angled brackets, (), denote concatenation.

3.1. Sequence to Sequence Generation

We develop our model based on the standard
Seq2Seq (Sutskever et al., 2014) model with at-
tention mechanism (Bahdanau et al., 2014) and

"We release the code, datasets and model outputs for repro-
ducing our results in https: //github.com/memray/
OpenNMT-kpg- release.

pointer softmax (Giilcehre et al., 2016). Due to
space limit, we describe this basic Seq2Seq model
in Appendix A.

3.2 Mechanisms for Diverse Generation

There are usually multiple keyphrases for a given
source text because each keyphrase represents cer-
tain aspects of the text. Therefore keyphrase di-
versity is desired for the keyphrase generation.
Most previous keyphrase generation models gener-
ate multiple phrases by over-generation, which is
highly prone to generate similar phrases due to the
nature of beam search. Given our objective to gen-
erate variable numbers of keyphrases, we need to
adopt new strategies for achieving better diversity
in the output.

Recall that we represent variable numbers of
keyphrases as delimiter-separated sequences. One
particular issue we observed during error analysis
is that the model tends to produce identical tokens
following the delimiter token. For example, sup-
pose a target sequence contains n delimiter tokens
at time-steps t),...,t,. During training, the model
is rewarded for generating the same delimiter token
at these time-steps, which presumably introduces
much homogeneity in the corresponding decoder
states ni, eng hin. When these states are subse-
quently used as inputs at the time-steps immedi-
ately following the delimiter, the decoder naturally
produces highly similar distributions over the fol-
lowing tokens, resulting in identical tokens being
decoded. To alleviate this problem, we propose two
plug-in components for the sequential generation
model.

3.2.1 Semantic Coverage

We propose a mechanism called semantic coverage
that focuses on the semantic representations of gen-
erated phrases. Specifically, we introduce another
uni-directional recurrent model GRUgc (dubbed
target encoder) which encodes decoder-generated
tokens y7, where 7 € [0,t), into hidden states hig.
This state is then taken as an extra input to the
decoder GRU, modifying equation of the decoder
GRU to:

hi, = GRUa((2', hc), hi). (1)

If the target encoder were to be updated with the
training signal from generation (i.e., backpropagat-
ing error from the decoder GRU to the target en-
coder), the resulting decoder is essentially a 2-layer
GRU with residual connections. Instead, inspired