Weisfeiler and Leman go sparse

 

Table 4: Classification accuracies in percent and standard deviations, OOT— Computation did not finish within one day, OOM—
Out of memory.

 

 

 

 

 

 

 

  

Dataset
Method
ENZYMES IMDB-BINARY IMDB-MuLTI  NCII NCI109 PTC_FM PROTEINS REDDIT-BINARY

» GR 29.8 41.0 59.5 40.4 40.6 +0.5 66.3 40.2 66.7+0.2 62.3 40.9 71.6 +0.2 60.0 +0.2
2 SP 42.3 41.3 59.2 +0.3 39.6 +0.4 74.5 40.3 73.4401 63.2406 76.4 +0.4 84.7+40.2
4 1-WL 53.4 41.4 72.4 40.5 50.6 +0.6 85.1 40.2 85.2402 62.9416 73.7 £0.5 75.3 £0.3
™ WLOA 59.7 41.2 73.1 40.7 50.3 +0.6 85.6 +0.2 86.0403 63.7407 73.70.54 88.7 +0.2
z Gin-0 39.6 41.3 72.8 40.9 50.0 +0.1 78.5405 771406 580414 71.7 +09 90.7 +0.9
2 Gin-e 38.7 42.1 73.0 +1.0 49.8 +0.6 78.8 +0.3 77.2403 58.7417 70.4 +1.2 89.4 41.2
_ 2-WL 38.9 +0.8 69.2 +0.6 48.0 +0.5 67.5 40.3 68.3402 64.3 40.6 75.3 £0.3 OoM
s 3-WL 45.9 +0.8 69.2 +0.4 47.9 0.7 Oor OoT 64.4 +0.6 Oom Oom
© 5§-2-WL 39.1 41.1 69.4 +0.7 48.0 +0.4 67.4403 68.3403 645404 75.2 +05 OOM

6-3-WL 45.9 +0.9 69.1 +0.6 47.9 +0.8 OoT OoT 64.4 +0.6 Oom Oom

6-2-LWL 57.7 +1.0 73.3 £0.7 50.9 +0.6 85.4402 848402 62.7413 74.5 +06 90.0 +0.2
 5-2-LWLt+ 57.0 40.8 78.9 40.6 64.0 +0.4 91.840.2 90.8402 62.7414 82.6 +04 91.5 +0.2
4 6-3-LWL 60.4 +0.8 73.5 £0.5 49.6 0.7 84.0403 83.0403 62.6 41.2 Oom Oom

6-3-LWLt 58.9 41.1 80.6 +0.5 60.3 +0.4 83.9403 82.9403 62.4412 Oom Oom

 

Table 5: Classification accuracies in percent and standard deviations on medium-scale datasets.

 

 

 

 

Dataset
Method YEAST YEASTH UACC257 UACC257H OVCAR-8 OVCAR-8H
1-WL 88.9 <01 88.9<01 96.8 <0.1 96.9 < 0.1 96.3 <0.1 96.3 < 0.1
GINE 88.3 <01 88.3<01 95.9 <0.1 95.9 < 0.1 94.9 <0.1 94.9 < 0.1
GINE-e 88.3 <01 88.3<01 95.9 <01 95.9 < 0.1 95.0 < 0.1 94.9 < 0.1

 

6-2-LWL 88.6<01 88.5<01 96.8 <0.1 96.5 < 0.1 96.1 <0.1 95.9 < 0.1
6-2-LWLT 98.9 <01 99.1<0.1 99.2<01 98.9 <0.1 99.3 <0.1 99.0 < 0.1

Local Neural

 

encoding of the (labeled) isomorphism type. Finally, we used a 2-layer MLP to learn a joint, initial vectorial representation.

The source code of all methods and evaluation procedures will be made available at www. github.com/chrsmrrs/
sparsewl.

G.2. Experimental protocol and model configuration

In the following, we describe the experimental protocol and hyperparameter setup.

Kernels For the smaller datasets (first third of Table 3), for each kernel, we computed the (cosine) normalized gram matrix. We
computed the classification accuracies using the C-SVM implementation of LIBS VM (Chang & Lin, 2011), using 10-fold
cross-validation. The C-parameter was selected from {10~°, 10-7, ... , 107, 10°} by (inner) 10-fold cross-validation on
the training folds. We repeated each 10-fold cross-validation ten times with different random folds, and report average
accuracies and standard deviations. For the larger datasets (second third of Table 3), we computed sparse feature vectors for
each graph and used the linear C-SVM implementation of LIBLINEAR (Fan et al., 2008), using 10-fold cross-validation.
The C-parameter was again selected from {10~°, 10-?,..., 107, 10°} using a validation set sampled uniformly at
random from the training fold (using 10% of the training fold). For measuring the classification accuracy, the number of
iterations of the 1-WL, WLOA, 6-k-LWL, the 6-k-LWL*, and the k-WL were selected from {0,...,5} using 10-fold
cross validation on the training folds only, or using the validation set for the medium-scale datasets.'' Moreover, for
the 6-k-LWL*, we only added the additional label function # on the last iteration to prevent overfitting. We report

 

"As already shown in (Shervashidze et al., 2011), choosing the number of iterations too large will lead to overfitting.