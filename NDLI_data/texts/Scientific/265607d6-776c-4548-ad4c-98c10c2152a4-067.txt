2. Web Information Retrieval

 

The Text REtrieval Conference (TREC), one of the major forums for research
in information retrieval (Voorhees & Harman, 2005; Voorhees, 2007) can be seen
as a modern instantiation of the Cranfield paradigm. In particular, TREC was in-
troduced in 1992 in a co-sponsorship between the National Institute of Standards
and Technology (NIST) and the Defense Advanced Research Projects Agency
(DARPA), both U.S. government agencies. Since its inception, the conference
has witnessed a substantial increase in the number of participant groups working
on several different search scenarios (known as tracks in the TREC jargon).

The overall aim of TREC is to support information retrieval research by pro-
viding the necessary infrastructure for the evaluation of retrieval techniques on a
common benchmark, known as a test collection. A test collection comprises three
components: a corpus of documents, a set of stated information needs (called
topics), and a set of relevance assessments, which function as a mapping between
each topic and the documents deemed as relevant for this topic. A prototypical
TREC track works as follows (Voorhees, 2007). Firstly, a document corpus is
built so as to serve as a common testbed for experimentation in the particular
search scenario addressed by the track, such as web search. Secondly, NIST pro-
vides the participants with a set of topics representing realistic information needs
for the search scenario under consideration. Thirdly, in order to build a ground-
ruth for evaluating the participants’ approaches as to the extent to which they
are able to retrieve the relevant documents in the corpus for the devised topics
ahead of irrelevant ones, a process called pooling (Sparck Jones & van Rijsbergen,
975) is usually employed. This process consists of building a pool of documents
for each of the considered topics as the union of the top documents retrieved for
hat topic by all the participant systems. These document pools are then sampled
and submitted to manual relevance assessment. Finally, the participant groups

submit the document rankings (known as runs) generated by their different re-

 

rieval approaches for each of the considered topics. These document rankings
are then scored based on the produced relevance assessments according to several
standard evaluation metrics, such as those discussed in Section 2.3.3. By evalu-
ating the participants’ approaches using this common benchmark, TREC allows
for the direct comparison of their deployed ranking techniques, hence identifying

which techniques work best for the retrieval scenario under consideration.

49