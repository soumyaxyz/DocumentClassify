Weisfeiler and Leman go sparse

 

Table 3: Dataset statistics and properties, Y—Continuous vertex labels following (Gilmer et al., 2017), the last three components
encode 3D coordinates.

 

 

 

 

 

Properties
Dataset = z
Number of graphs Number of classes/targets | @ Number of vertices @ Number of edges Vertex labels Edge labels

ENZYMES 600 6 32.6 62.1 v x
IMDB-BINARY 1000 2 19.8 96.5 x x
IMDB-MULTI 1500 3 13.0 65.9 x x
NCI1 4110 2 29.9 323 v x
NCI109 4127 2 29.7 32.1 v x
PTC_FM 349 2 14.1 14.5 v x
PROTEINS 1113 2 39.1 72.8 v x
REDDIT-BINARY 2000 2 429.6 497.8 x x
YEAST 79601 2 21.5 22.8 v v
YEASTH 79601 2 39.4 40.7 v v
UACC257 39 988 2 26.1 28.1 v v
UACC257H 39 988 2 46.7 48.7 v v
OVCAR-8 40516 2, 26.1 28.1 v v
OVCAR-8H 40516 2 46.7 48.7 v v
ZINC 249 456 12 23.1. 24.9 v v
ALCHEMY 202 579 12 10.1 10.4 v v
QmM9 129 433 12 18.0 18.6 ¥(13+3D)t v(4)

 

dataset.® To study data efficiency, we also used smaller subsets of the ZINC and ALCHEMY dataset. That is, for the ZINC
10K (ZINK 50K) dataset, following (Dwivedi et al., 2020), we sampled 10000 (50000) graphs from the training, and 1 000
(5.000) from the training and validation split, respectively. For ZINC 10K, we used the same splits as provided by (Dwivedi
et al., 2020). For the ALCHEMY 10K (ALCHEMY 50K) dataset, as there is no fixed split available for the full dataset®, we
sampled the (disjoint) training, validation, and test splits uniformly and at random from the full dataset. See Table 3 for
dataset statistics and properties.!°

Kernels We implemented the 5-k-LWL, 5-k-LWL*, 6-k-WL, and k-WL kernel for k in {2,3}. We compare our kernels to the
Weisfeiler-Leman subtree kernel (1-WL) (Shervashidze et al., 2011), the Weisfeiler-Leman Optimal Assignment kernel
(WLOA) (Kriege et al., 2016), the graphlet kernel (Shervashidze et al., 2009) (GR), and the shortest-path kernel (Borgwardt
& Kriegel, 2005) (SP). All kernels were (re-)implemented in C++1 1. For the graphlet kernel we counted (labeled) connected
subgraphs of size three.

Neural architectures We used the GIN and GIN-e architecture (Xu et al., 2019) as neural baselines. For data with (continuous)
edge features, we used a 2-layer MLP to map them to the same number of components as the node features and combined
them using summation (GINE and GINE-). For the evaluation of the neural architectures of Appendix E, 5-k-LGNN, 5-k-
GNN, k-WL-GNN, we implemented them using PYTORCH GEOMETRIC (Fey & Lenssen, 2019), using a Python-wrapped
C++11 preprocessing routine to compute the computational graphs for the higher-order GNNs. We used the GIN-e layer to
express Jeti and fai of Equation (7). Finally, we used the PYTORCH (Paszke et al., 2019) implementations of the
3-IGN (Maron et al., 2019a), and 1-2-GNN, 1-3-GNN, 1-2-3-GNN (Morris et al., 2019) made available by the respective
authors.

For the QM9 dataset, we additionally used the MPNN architecture as a baseline, closely following the setup of (Gilmer
et al., 2017). For the GINE-e and the MPNN architecture, following Gilmer et al. (Gilmer et al., 2017), we used a complete
graph, computed pairwise ¢2 distances based on the 3D-coordinates, and concatenated them to the edge features. We note
here that our intent is not the beat state-of-the-art, physical knowledge-incorporating architectures, e.g., DimeNet (Klicpera
et al., 2020) or Cormorant (Anderson et al., 2019), but to solely show the benefits of the (local) higher-order architectures
compared to the corresponding (1-dimensional) GNN. For the 6-2-GNN, to implement Equation (8), for each 2-tuple we
concatenated the (two) node and edge features, computed pairwise £2 distances based on the 3D-coordinates, and a one-hot

’We opted for comparing on the QM9 dataset to ensure a fair comparison concerning hyperparameter selection.

°Note that the full dataset is different from the contest dataset, e.g., it does not provide normalized targets, see https: //alchemy.
tencent.com/.

'° All datasets can be obtained from ht tp: //www.graphlearning. io.