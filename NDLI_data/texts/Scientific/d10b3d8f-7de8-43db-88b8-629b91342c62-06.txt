sGNN(I, A)

!

cent eee

SDP gh MPNN* sGNN(I, D, A, {min{A‘, 1}}21)

order 2 G-invariant networks*

   
 
  

spectral methods

SoS hierarchy Ring-GNN

Figure 1: Relative comparison of function class

 

's in terms of their ability to solve graph isomorphism.

“Note that, on one hand GIN is defined by a form of message passing neural network justifying the
inclusion GIN + MPNN. On the other hand shows that mess assing neural networks can be expressed
as a modified form of order 2 G-invariant networks (which may not coincide with the definition we consider in
this paper). Therefore the inclusion GIN — order 2 G-invariant networks has yet to be established rigorously.

5 Ring-GNN: a GNN defined on the ring of equivariant functions

 

We now investigate the G-invariant network framework proposed in (see Appendix [D] for its
definition and a description of an adapted version that works on graph-structured inputs, which
we call the Graph G-invariant Networks). The architecture of G-invariant networks is built by
interleaving compositions of equivariant linear layers between tensors of potentially different orders
and point-wise nonlinear activation functions. It is a powerful framework that can achieve universal

approximation if the order of the tensor can grow as nlns —) , where n is the number of nodes in the
graph, but less is known about its approximation power when the tensor order is restricted. One
particularly interesting subclass of G-invariant networks is the ones with maximum tensor order 2,
because shows that it can approximate any Message Passing Neural Network [8]. Moreover, it is
both mathematically cumbersome and computationally expensive to include equivariant linear layers
involving tensors with order higher than 2.

 

Our following result shows that the order-2 Graph G-invariant Networks subclass of functions is
quite restrictive. The proof is given in Appendix[D}

Theorem 7. Order-2 Graph G-invariant Networks cannot distinguish between non-isomorphic
regular graphs with the same degree.

Motivated by this limitation, we propose a GNN architecture that extends the family of order-2 Graph
G-invariant Networks without going into higher order tensors. In particular, we want the new family
to include GNNs that can distinguish some pairs of non-isomorphic regular graphs with the same
degree. For instance, take the pair of Circular Skip Link graphs G'g,2 and G'g.3, illustrated in Rigue
Roughly speaking, if all the nodes in both graphs have the same node feature, then because they a
have the same degree, the updates of node states in both graph neural networks based on neighborhood
aggregation and the WL test will fail to distinguish the nodes. However, the power graphs" of Gg.o
and Gg.3 have different degrees. Another important example comes from spectral methods that
operate on normalized operators, such as the normalized Laplacian A = I — D~!/? AD~1/2, where
D is the diagonal degree operator. Such normalization preserves the permutation symmetries and in
many clustering applications leads to dramatic improvements I

This motivates us to consider a polynomial ring generated by the matrices that are the outputs of
permutation-equivariant linear layers, rather than just the linear space of those outputs. Together
with point-wise nonlinear activation functions such as ReLU, power graph adjacency matrices like
min(A?, 1) can be expressed with suitable choices of parameters. We call the resulting architecture
the Ring-GNNF|

"If A is the adjacency matrix of a graph, its power graph has adjacency matrix min(A?, 1). The matrix
min(A?, 1) has been used in [5] in graph neural networks for community detection and in [21] for the quadratic
assignment problem.

*We call it Ring-GNN since the main object we consider is the ring of matrices, but technically we can
express an associative algebra since our model includes scalar multiplications.