Antofagasta or)

 

V7
me T. of
eA, er
atx L-west —s ~ — ey es >
north of north of Tno.

e ec.
(Licantén }—west of—»| Curico | / *

(a) Original graph (b) Relation embeddings (c) Entity embeddings

 

 

 

 

 

Fig. 24. Toy example of two-dimensional relation and entity embeddings learnt by TransE; the entity embed-
dings use abbreviations and include an example of vector addition to predict what is west of Antofagasta

will tend to cancel each other out. To resolve such issues, many variants of TransE have been
investigated. Amongst these, for example, TransH [521] represents different relations using distinct
hyperplanes, where for the edge @-p-+(0), (is first projected onto the hyperplane of p before the
translation to ©) is learnt (uninfluenced by edges with other labels for @ and for (0). TransR [256]
generalises this approach by projecting (3) and (9) into a vector space specific to p, which involves
multiplying the entity embeddings for (8) and () by a projection matrix specific to p. TransD [256]
simplifies TransR by associating entities and relations with a second vector, where these secondary
vectors are used to project the entity into a relation-specific vector space. For discussion of other
translational models, we refer to the survey by Wang et al. [519].

5.2.2 Tensor decomposition models. A second approach to derive graph embeddings is to apply
methods based on tensor decomposition. A tensor is a multidimensional numeric field that generalises
scalars (0-order tensors), vectors (1-order tensors) and matrices (2-order tensors) towards arbitrary
dimension/order. Tensors have become a widely used abstraction for machine learning [402].
Tensor decomposition involves decomposing a tensor into more “elemental” tensors (e.g., of lower
order) from which the original tensor can be recomposed (or approximated) by a fixed sequence of
basic operations. These elemental tensors can be viewed as capturing latent factors underlying the
information contained in the original tensor. There are many approaches to tensor decomposition,
where we will now briefly introduce the main ideas behind rank decompositions [402].

Leaving aside graphs momentarily, consider an (a, b)-matrix (i.e., an order-2 tensor) C, where a
is the number of cities in Chile, b is the number of months in a year, and each element (C);; denotes
the average temperature of the i'" city in the j'" month. Noting that Chile is a long, thin country
— ranging from subpolar climates in the south, to a desert climate in the north — we may find a
decomposition of C into two vectors representing latent factors — specifically x (with m elements)
giving lower values for cities with lower latitude, and y (with n elements), giving lower values for
months with lower temperatures - such that computing the outer product”! of the two vectors
approximates C reasonably well: x @ y ~ C. In the (unlikely) case that there exist vectors x and
y such that that C is precisely the outer product of two vectors (x ® y = C) we call C a rank-1
matrix; we can then precisely encode C using m + n values rather than m x n values. Most times,
however, to get precisely C, we will need to sum multiple rank-1 matrices, where the rank r of C is
the minimum number of rank-1 matrices that need to be summed to derive precisely C, such that

24The outer product of two (column) vectors x of length m and y of length n, denoted x ® y, is defined as xy’, yielding an
(m, n)-matrix M such that (M)j; = (x); - (y);. Analogously, the outer product of k vectors is a k-order tensor.

38