| Present | Absent

 

Model | F:@5 | F,@10 | F:@O | R@1O | R@S0

 

 

 

 

Tfldf | 8.0 8.9 52
TextRank | 12.1 10.1 11.6 -
KEA | 4.9 48 53 -
Maui | 35.8 23.3 51.8
CopyRNN* | 44.2 30.3 66.2 48.8 66.0
catSeq | 48.3 45.5 63.5 40.7 42.2
catSeqp | 48.7 43.9 65.6 54.8 65.7

 

 

Table 3: Model performance on STACKEx dataset.

This includes four non-neural extractive models:
Tfldf (Hasan and Ng, 2010), TextRank (Mihalcea
and Tarau, 2004), KEA (Witten et al., 1999), and
Maui (Medelyan et al., 2009); one neural extractive
model (Sun et al., 2019); and two neural models
that use additional data (e.g., title) (Ye and Wang,
2018; Chen et al., 2019b).

In Section 5.3, we apply the self-terminating de-
coding strategy. Since no existing model supports
such decoding strategy, we only report results from
our proposed models. They can be used for com-
parison in future studies.

5.1 Experiments on Scientific Publications

Our first dataset consists of a collection of scientific
publication datasets, namely KP20K, INSPEC,
KRAPIVIN, NUS, and SEMEVAL, that have been
widely used in existing literature (Meng et al., 2017;
Chen et al., 2018a; Ye and Wang, 2018; Chen et al.,
2018b; Chan et al., 2019; Zhao and Zhang, 2019;
Chen et al., 2019a; Sun et al., 2019). KP20k, for
example, was introduced by Meng et al. (2017)
and comprises more than half a million scientific
publications. For each article, the abstract and title
are used as the source text while the author key-
words are used as target. The other four datasets
contain much fewer articles, and thus used to test
transferability of our model.

We report our modelâ€™s performance on the
present-keyphrase portion of the KP20x dataset in
Table 2.4 To compare with previous works, we pro-
vide compute f@5 and F@10 scores. The new
proposed F; @O metric indicates consistent rank-
ing with F,@5/10 for most cases. Due to its target
number sensitivity, we find that its value is closer to
F\@5 for KP20x and KRAP IVIN where average
target keyphrases is less and closer to F; @10 for
the other three datasets.

*We show experiment results on absent data in Ap-
pendix B.

 

| KP20K | STACKEX

 

Model | F,@O | F,@M | F:@O | Fi@M

 

 

Greedy Search

 

CatSeq | 33.1 | 32.4 | 59.2 | 56.3
59.6

CatSeqD 33.4 33.9 59.3

 

 

Top Ranked Sequence in Beam Search

 

catSeq
CatSeqD

 

24.3 |

52.4 52.7
31.9

57.0

 

 

Table 4: F; @O and F; @M when generating variable
number of keyphrases (self-terminating decoding).

From the result we can see that our Cat SeqD
outperform existing abstractive models on most
of the datasets. Our implemented CopyRNN*
achieves better or comparable performance against
the original model, and on NUS and SemEval the
advantage is more salient.

As for the proposed models, both Cat Seq and
CatSeqD yield comparable results to CopyRNN,
indicating that ONE2SEQ paradigm can work well
as an alternative option for the keyphrase genera-
tion task. Cat SeqD outperforms Cat Seq on all
metrics, suggesting the semantic coverage and or-
thogonal regularization help the model to generate
higher quality keyphrases and achieve better gener-
alizability. To our surprise, on the metric F; @10
for KP20x and KRAPIVIN (average number of
keyphrases is only 5), where high-recall models
like CopyRNN are more favored, Cat SeqD is still
able to outperform ONE 2ONE baselines, indicating
that the proposed mechanisms for diverse genera-
tion are effective.

5.2 Experiments on The StackEx Dataset

Inspired by the StackLite tag recommendation task
on Kaggle, we build a new benchmark based on
the public StackExchange data*. We use questions
with titles as source, and user-assigned tags as tar-
get keyphrases. We provide details regarding our
data collection in Appendix C.

Since oftentimes the questions on StackEx-
change contain less information than in scientific
publications, there are fewer keyphrases per data
point in STACKEx (statistics are shown in Table 1).
Furthermore, StackExchange uses a tag recommen-
dation system that suggests topic-relevant tags to
users while submitting questions; therefore, we
are more likely to see general terminology such as

>https://archive.org/details/stackexchange, we choose 19
computer science related topics from Oct. 2017 dump.