Context Vector

Attention
Distribution
=

"beat"

L___,
Wiese aa >z00 }

 

uonnqiisiq
Aaeinqesop,

 

Encoder
Hidden

{HH

Germany emerge victorious in 20 win

States

 

against Argentina on

b., \

7

saqelg UapplH
Japooag

Saturday. <START> Germany

 

\
Source Text

W—_\—’

Partial Summary

Figure 2: Baseline sequence-to-sequence model with attention. The model may attend to relevant words
in the source text to generate novel words, e.g., to produce the novel word beat in the abstractive summary
Germany beat Argentina 2-0 the model may attend to the words victorious and win in the source text.

et al., 2014), in which recurrent neural networks
(RNNs) both read and freely generate text, has
made abstractive summarization viable (Chopra
et al., 2016; Nallapati et al., 2016; Rush et al.,
2015; Zeng et al., 2016). Though these systems
are promising, they exhibit undesirable behavior
such as inaccurately reproducing factual details,
an inability to deal with out-of-vocabulary (OOV)
words, and repeating themselves (see Figure 1).

In this paper we present an architecture that
addresses these three issues in the context of
multi-sentence summaries. While most recent ab-
stractive work has focused on headline genera-
tion tasks (reducing one or two sentences to a
single headline), we believe that longer-text sum-
marization is both more challenging (requiring
higher levels of abstraction while avoiding repe-
tition) and ultimately more useful. Therefore we
apply our model to the recently-introduced CNN/
Daily Mail dataset (Hermann et al., 2015; Nallap-
ati et al., 2016), which contains news articles (39
sentences on average) paired with multi-sentence
summaries, and show that we outperform the state-
of-the-art abstractive system by at least 2 ROUGE
points.

Our hybrid pointer-generator network facili-
tates copying words from the source text via point-
ing (Vinyals et al., 2015), which improves accu-
racy and handling of OOV words, while retaining
the ability to generate new words. The network,
which can be viewed as a balance between extrac-
tive and abstractive approaches, is similar to Gu
et al.’s (2016) CopyNet and Miao and Blunsom’s
(2016) Forced-Attention Sentence Compression,

that were applied to short-text summarization. We
propose a novel variant of the coverage vector (Tu
et al., 2016) from Neural Machine Translation,
which we use to track and control coverage of the
source document. We show that coverage is re-
markably effective for eliminating repetition.

2 Our Models

In this section we describe (1) our baseline
sequence-to-sequence model, (2) our pointer-
generator model, and (3) our coverage mechanism
that can be added to either of the first two models.
The code for our models is available online.!

2.1 Sequence-to-sequence attentional model

Our baseline model is similar to that of Nallapati
et al. (2016), and is depicted in Figure 2. The to-
kens of the article w; are fed one-by-one into the
encoder (a single-layer bidirectional LSTM), pro-
ducing a sequence of encoder hidden states h;. On
each step t, the decoder (a single-layer unidirec-
tional LSTM) receives the word embedding of the
previous word (while training, this is the previous
word of the reference summary; at test time it is
the previous word emitted by the decoder), and
has decoder state s,. The attention distribution a’
is calculated as in Bahdanau et al. (2015):

ef = v" tanh(Wihi + Wos; + Daun) (1)

a = softmax(e') (2)

where v, W,, W,; and Dat are learnable parame-
ters. The attention distribution can be viewed as

'yww.github.com/abisee/pointer-generator