et al., 2014) to compute an attention score a;,; for
each of the word 2; in the document. The attention
scores are next used to compute a context vector
hj for the document. The probability of predicting
a word y; from a predefined vocabulary V is de-
fined as Py(y:) = softmax(Wy (Wy-|s;; h7])).
In this paper, all the W terms represent trainable
parameters and we omit the bias terms for brevity.

Pointer-generator network. To alleviate the
out-of-vocabulary (OOV) problem, we adopt the
copy mechanism from See et al. (2017). For
each document x, we build a dynamic vocabu-
lary V,;. by merging the predefined vocabulary V
and all the words that appear in x. Then, the
probability of predicting a word y; from the dy-
namic vocabulary Vx is computed as Py, (y%:) =
DgenPv (Yt) + et _ Pgen)Pa( yt)» where Pe(yt) =
Viewimye az, is the copy distribution and pgen =
sigmoid(W,,[h;; s;; e:-1]) € [0, 1] isa soft gate to
select between generating a word from the vocab-
ulary V and copying a word from the document.

Maximum likelihood training. We use @ to de-
note all model parameters and y;_; to denote
a sequence (y1,...,ys—1). Previous work learns
the parameters by maximizing the log-likelihood
of generating the ground-truth output sequence y,
defined as follows,

Ly

L(0) = — Slog Pg (velyiv—1, x38).
t=1

4 Reinforcement Learning Formulation

We formulate the task of keyphrase generation as
a reinforcement learning problem, in which an
agent interacts with an environment in discrete
time steps. At each time step t = 1,...,7', the
agent produces an action (word) % sampled from
the policy 7(%|¥1-1-1, x; 0), where ¥1.4_1 denotes
the sequence generated by the agent from step 1 to
t— 1. After that, the environment gives a reward
rt(¥1, Y) to the agent and transits to the next step
t+1 with a new state §:41 = (¥1, x, Y). The pol-
icy of the agent is a keyphrase generation model,
ie., 7(.|¥1t—-1,%5 9) = Pry (-|¥1t—-1, 5 9).

To improve the sufficiency and accuracy of
both present keyphrases and absent keyphrases
generated by the agent, we give separate reward
signals to present keyphrase predictions and ab-
sent keyphrase predictions. Hence, we divide
our RL problem into two different stages. In

the first stage, we evaluate the agent’s perfor-
mance on extracting present keyphrases. Once
the agent generates the ‘o’ token, we denote the
current time step as TJ”, the environment com-
putes a reward using our adaptive reward func-
tion RF, by comparing the generated keyphrases
in ¥,,7p with the ground-truth present keyphrases
Y? ie, ree (Yipe, VY) = RFi(¥-re, Y?). Then
we enter the second stage, where we evalu-
ate the agent’s performance on generating ab-
sent keyphrases. Upon generating the EOS to-
ken, the environment compares the generated
keyphrases in Yrr4,-7 with the ground-truth
absent keyphrases Y° and computes a reward
rr(¥ur, Y) = RE\(¥re41:7, Y*). After that, the
whole process terminates. The reward to the agent
is 0 for all other time steps, i-e., ri(yiu,Y) = 0
for allt ¢ {T?, T}.

Let return R,(y,)/) be the sum of future re-
ward starting from time step t, ie, Ri(y¥,Y) =
wy, r7(Y1:r,Y), where y denotes the complete
sequence generated by the agent, ie., y = Yur.
We then simplify the expression of return into:

RFi(¥re, Y?)+

R= RE\(Yrregiur,Y*) ifl<t<T?,
RA(Vreyir,%) if T? <t<T.
(2)

The goal of the agent is to maximize the
expected initial return Ey Jr jx,9)Ri(y,¥),
where Ri(y,V) = RAY .pr.¥?) +
RAYre sir. Y").

Adaptive reward function. To encourage
the model to generate sufficient and accurate
keyphrases, we define our adaptive reward func-
tion RF as follows. First, let N be the number
of predicted keyphrases, and G be the number of
ground-truth keyphrases, then

Eh = va ifN <G, @

 

 

 

 

Fy otherwise.

If the model generates insufficient number of
keyphrases, the reward will be the recall of the
predictions. Since generating incorrect keyphrases
will not decrease the recall, the model is encour-
aged to produce more keyphrases to boost the re-
ward. If the model generates a sufficient number
of keyphrases, the model should be discouraged
from over-generating incorrect keyphrases, thus

2166