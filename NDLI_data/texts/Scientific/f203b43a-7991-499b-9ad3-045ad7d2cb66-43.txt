EIRINI PAPAGIANNOPOULOU AND GRIGORIOS TSOUMAKAS 43

 

To sum up, it is strongly recommended to the research community to explicitly mention which set of keyphrases
is regarded as a gold evaluation standard in the experiments, since different evaluation gold standards give differ-
ent performance ranking and score estimations. Particularly, the union of the keyphrases evaluation gold standard
is strongly correlated to the readers’ evaluation gold standard for all keyphrase extraction methods in both datasets.
Moreover, the average F;@10 scores with respect to readers’ evaluation gold standard seem to be closer to the scores
with respect to the union's evaluation gold standard, even though in most cases at the Semeval dataset and in half
cases of the NUS dataset the union’s and the readers’ evaluation gold standards give scores coming from different dis-
tributions. Using both types of annotations, i.e., authors’ keyphrases and multiple annotators’ (readers’) keyphrases,
we have at our disposal a quite expanded set of keyphrases. However, using only the multiple annotators’ keyphrases,
we end up to a decent number of gold unbiased keyphrases certainly depending on the methodology followed for the
annotation process and the collective effort. On the other hand, authors’ sets of keyphrases contain fewer but suffi-
cient phrases to cover the topics of the target document. Such type of evaluation standard usually gives lower per-
formance scores, moderately correlated with those resulting from the union keyphrases’ gold evaluation standard. Fi-
nally, the intersection’s evaluation gold standard is not recommended, as it is quite strict, contains a very small number
of keyphrases, and does not guarantee that all topics discussed in the target documents are covered by the intersec-
tion of the keyphrases, since authors and readers may use different expressive means/vocabularies and do not share
same annotation motivations.

6.6 | Qualitative Analysis

Via this qualitative study, we would like to highlight the limitations of the exact match evaluation, especially in cases
where we are interested in the actual performance (success rate) of a method. Additionally, we show a case where
the “looser” partial match strategy gives lower scores than the exact match one. Finally, we give an example that the
partial match evaluation can be harmful, failing to assess properly the syntactic correctness of the return keyphrases.
We use MR to extract the keyphrases from 3 publication full-texts of the Krapivin dataset collection.

First, we present an example where the F;-score with respect to the partial evaluation is greater than the cor-
responding exact match score and closer to the corresponding score with respect to the manual evaluation, which
intuitively is the expected case. We quote the publication’s title and abstract below in order to get a sense of its con-

tent:

 

Title: Exact algorithms for finding minimum transversals in rank-3 hypergraphs.
Abstract: We present two algorithms for the problem of finding a minimum transversal in a hypergraph of rank
3, also known as the 3-Hitting Set problem. This problem is a natural extension of the vertex cover problem for

ordinary graphs. The first algorithm runs in time O(1.6538n) for a hypergraph with n vertices, and needs polynomial

 

 

space. The second algorithm uses exponential space and runs in time O(1.6316n).

 

The corresponding set of the “gold” keyphrases are: {hypergraph, 3-hitting set, exact algorithm, minimum transversal}.
For evaluation purposes, we transform the set of “gold” keyphrases into the following one (after stemming and removal
of punctuation marks, such as dashes and hyphens):
{(hypergraph), (3hit, set), (exact, algorithm), (minimum, transvers)}
The MR’s result set is given in the first box below, followed by its stemmed version in the second box. The words
that are both in the golden set and in the set of our candidates are highlighted with bold typeface: